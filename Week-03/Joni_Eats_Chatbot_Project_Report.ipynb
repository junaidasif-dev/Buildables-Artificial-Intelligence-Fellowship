{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7503624",
   "metadata": {},
   "source": [
    "# Theoretical Report – Joni Eats Chatbot (Groq + Streamlit)\n",
    "This section answers the guideline’s questions (1–6) in narrative form for reporting and submission.\n",
    "\n",
    "## 1) Use Case, Target Users, Benefits, Limitations\n",
    "Use case: A customer-facing cafe assistant for Joni Eats. The bot answers menu questions, dietary info (vegan/halal/gluten‑free), opening hours, location, specials, wait times, and simple ordering guidance.\n",
    "\n",
    "- Target users: Walk‑in customers, regulars, and first‑time visitors who want quick answers without waiting for staff during rush hours.\n",
    "- Expected benefits: Faster service, fewer repetitive questions for staff, consistent information, and better discovery of items (e.g., seasonal specials).\n",
    "- Limitations: Not a payment system, no real‑time inventory unless integrated, may not reflect last‑second menu changes, and should not give medical or legal advice.\n",
    "\n",
    "## 2) Justification\n",
    "A chatbot is well‑suited because cafe inquiries are short, frequent, and repetitive. Compared to browsing a website/menu PDF, chat supports quick follow‑ups (e.g., “any nut‑free desserts?”) and clarifications. Versus human staff, the bot can answer 24/7 and reduce queue pressure while keeping tone consistent. It will perform retrieval‑augmented Q&A from a curated knowledge base (menu, policies, specials) and follow a conversation flow for greetings, clarifying questions, and goodbyes.\n",
    "\n",
    "## 3) Groq Model Selection\n",
    "Primary model: llama3‑8b‑8192 on Groq for fast, low‑latency chat. Rationale:\n",
    "- Size/speed: 8B is cost‑effective and responsive for real‑time interactions; upgrade path to 70B exists for tougher reasoning.\n",
    "- Language: Good English fluency; multilingual support acceptable for basic hospitality queries.\n",
    "- Latency: Groq’s inference speed improves user experience during live chat.\n",
    "\n",
    "## 4) Conversation Flow (with samples)\n",
    "Flow:\n",
    "1. Greeting → “Hi, welcome to Joni Eats! How can I help?”\n",
    "2. Understand intent → Identify if the user asks about menu, hours, location, dietary needs, or specials.\n",
    "3. Retrieve context → Pull top KB snippets relevant to the query.\n",
    "4. Respond → Provide concise, friendly answer; list 3–5 options if appropriate.\n",
    "5. Clarify → Offer follow‑ups: “Want vegan or gluten‑free options?”\n",
    "6. Goodbye → Close politely or handoff to staff if needed.\n",
    "\n",
    "Sample inputs/responses:\n",
    "- Q: “Do you have vegan lunch options under $12?”\n",
    "  A: “Yes—try our Vegan Power Bowl and Roasted Veggie Wrap (both under $12). Would you like a protein add‑on?”\n",
    "- Q: “What time do you open on Saturdays?”\n",
    "  A: “We open at 8:00 AM on Saturdays and close at 8:00 PM.”\n",
    "\n",
    "## 5) Prompting Technique\n",
    "Technique: Few‑shot prompting (examples from the corpus’s CHAT_PATTERNS section) plus zero‑shot fallback. The system prompt defines the Joni Eats persona and scope. Structured, bulleted outputs keep answers skimmable. The prompt asks the model to only answer cafe‑related questions and to gracefully decline out‑of‑scope queries while steering back to menu/hours/specials.\n",
    "\n",
    "## 6) Frontend\n",
    "A Streamlit UI makes the bot easy to try: chat interface, settings (model, temperature, top‑k), and an expander that shows retrieved snippets for transparency. This supports quick feedback from staff and customers before deeper integrations (ordering, inventory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e113325",
   "metadata": {},
   "source": [
    "# Joni Eats Chatbot – Project Report\n",
    "A Groq LLM chatbot with a Streamlit frontend for the cafe \"Joni Eats.\" This notebook documents design choices, implementation steps, and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea15c33",
   "metadata": {},
   "source": [
    "## 1) Verify Environment and Install Dependencies\n",
    "Use %pip to install into the active kernel (ensure your .venv is selected in VS Code). We'll install: groq, streamlit, python-dotenv, scikit-learn, numpy, pandas, tiktoken (optional), pytest. Then quick import check and versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5672427",
   "metadata": {},
   "source": [
    "## 2) Load Secrets and Paths\n",
    "Load GROQ_API_KEY from the workspace .env using dotenv. Define paths for Week-03 assets and output directories. Validate existence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326268a7",
   "metadata": {},
   "source": [
    "## 3) Ingest Knowledge Base and Prompt Assets\n",
    "Read KB chunks and load prompt assets for templating; preview a few lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c5026",
   "metadata": {},
   "source": [
    "## 4) Build Lightweight Retriever (TF‑IDF cosine)\n",
    "Implement TF‑IDF vectorization and cosine similarity retrieval. Cache artifacts with joblib to accelerate reruns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c816133",
   "metadata": {},
   "source": [
    "## 5) Define System Prompt and Prompting Strategy\n",
    "Persona: You are Joni Eats’ friendly, concise, and helpful cafe assistant. Use few‑shot patterns from the corpus’s CHAT_PATTERNS section and context flow guidance from the CONTEXT_FLOW section; fall back to zero‑shot if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe5924",
   "metadata": {},
   "source": [
    "## 6) Initialize Groq Client and Sanity Check\n",
    "Create a client and send a tiny test to verify credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38b3f0",
   "metadata": {},
   "source": [
    "## 7) Compose RAG Pipeline: retrieve‑then‑generate\n",
    "Combine retriever with Groq to answer queries using KB snippets and few‑shot patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10629b",
   "metadata": {},
   "source": [
    "## 8) Chat Memory and Safety Guards\n",
    "Maintain rolling history and simple guardrails to keep the bot on-topic and safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "769e9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Backend Code (single cell)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# --- Paths & Secrets ---\n",
    "\n",
    "workspace_root = Path(\"d:/fellowship/Buildables-Artificial-Intelligence-Fellowship\").resolve()\n",
    "\n",
    "env_path = workspace_root / \".env\"\n",
    "\n",
    "load_dotenv(env_path)\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "week03 = workspace_root / \"Week-03\"\n",
    "\n",
    "corpus_path = week03 / \"joni_eats_corpus.txt\"\n",
    "\n",
    "cache_dir = week03 / \".cache\"\n",
    "\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# --- Helpers: IO & Parsing ---\n",
    "\n",
    "def read_text(path: Path) -> str:\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "\n",
    "def parse_corpus(corpus: str) -> tuple[str, str, str]:\n",
    "\n",
    "    # Split the combined corpus into sections\n",
    "\n",
    "    sections = re.split(r\"^### SECTION: (.+)$\", corpus, flags=re.M)\n",
    "\n",
    "    mapping = {\"RESTAURANT_KB\": \"\", \"CHAT_PATTERNS\": \"\", \"CONTEXT_FLOW\": \"\"}\n",
    "\n",
    "    for i in range(1, len(sections), 2):\n",
    "\n",
    "        name = sections[i].strip().upper()\n",
    "\n",
    "        body = sections[i+1] if i+1 < len(sections) else \"\"\n",
    "\n",
    "        if name in mapping:\n",
    "\n",
    "            mapping[name] = body.strip()\n",
    "\n",
    "    return mapping[\"RESTAURANT_KB\"], mapping[\"CHAT_PATTERNS\"], mapping[\"CONTEXT_FLOW\"]\n",
    "\n",
    "\n",
    "\n",
    "def split_into_chunks(text: str, min_len: int = 250) -> list[str]:\n",
    "\n",
    "    parts = [p.strip() for p in re.split(r\"\\n\\s*\\n+\", text) if p.strip()]\n",
    "\n",
    "    chunks: list[str] = []\n",
    "\n",
    "    for p in parts:\n",
    "\n",
    "        if len(p) >= min_len:\n",
    "\n",
    "            chunks.append(p)\n",
    "\n",
    "        else:\n",
    "\n",
    "            if chunks and len(chunks[-1]) < min_len:\n",
    "\n",
    "                chunks[-1] = chunks[-1] + \"\\n\" + p\n",
    "\n",
    "            else:\n",
    "\n",
    "                chunks.append(p)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def build_dietary_index(kb_text: str) -> dict:\n",
    "\n",
    "    idx = {\"vegan\": set(), \"vegetarian\": set(), \"gluten-free\": set(), \"halal\": set()}\n",
    "\n",
    "    for line in [l.strip() for l in kb_text.splitlines() if l.strip()]:\n",
    "\n",
    "        low = line.lower()\n",
    "\n",
    "        if any(k in low for k in idx.keys()) or \"veggie\" in low or \"gluten free\" in low:\n",
    "\n",
    "            if \"vegan\" in low:\n",
    "\n",
    "                idx[\"vegan\"].add(line)\n",
    "\n",
    "            if \"vegetarian\" in low or \"veggie\" in low:\n",
    "\n",
    "                idx[\"vegetarian\"].add(line)\n",
    "\n",
    "            if \"gluten-free\" in low or \"gluten free\" in low:\n",
    "\n",
    "                idx[\"gluten-free\"].add(line)\n",
    "\n",
    "            if \"halal\" in low:\n",
    "\n",
    "                idx[\"halal\"].add(line)\n",
    "\n",
    "    return {k: sorted(v) for k, v in idx.items()}\n",
    "\n",
    "\n",
    "\n",
    "# --- Load corpus & prepare chunks ---\n",
    "\n",
    "corpus_text = read_text(corpus_path)\n",
    "\n",
    "kb_text, chat_patterns, context_flow = parse_corpus(corpus_text)\n",
    "\n",
    "kb_chunks = split_into_chunks(kb_text, min_len=250)\n",
    "\n",
    "diet_index = build_dietary_index(kb_text)\n",
    "\n",
    "\n",
    "\n",
    "# --- Retriever (TF-IDF + cosine) with caching ---\n",
    "\n",
    "def build_tfidf_retriever(chunks: list[str], ngram_range=(1, 2)) -> tuple[TfidfVectorizer, any]:\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, stop_words=\"english\")\n",
    "\n",
    "    matrix = vectorizer.fit_transform(chunks)\n",
    "\n",
    "    return vectorizer, matrix\n",
    "\n",
    "\n",
    "\n",
    "cache_vec = cache_dir / \"tfidf_vectorizer.joblib\"\n",
    "\n",
    "cache_mat = cache_dir / \"tfidf_matrix.joblib\"\n",
    "\n",
    "if cache_vec.exists() and cache_mat.exists():\n",
    "\n",
    "    vectorizer = joblib.load(cache_vec)\n",
    "\n",
    "    matrix = joblib.load(cache_mat)\n",
    "\n",
    "else:\n",
    "\n",
    "    vectorizer, matrix = build_tfidf_retriever(kb_chunks)\n",
    "\n",
    "    joblib.dump(vectorizer, cache_vec)\n",
    "\n",
    "    joblib.dump(matrix, cache_mat)\n",
    "\n",
    "\n",
    "\n",
    "def _expand_query(query: str) -> str:\n",
    "\n",
    "    ql = query.lower()\n",
    "\n",
    "    if any(t in ql for t in [\"veg \", \" veg\", \"vegan\", \"vegetarian\", \"veggie\", \"plant based\", \"plant-based\"]):\n",
    "\n",
    "        query = query + \" vegan vegetarian veggie plant-based meatless dairy-free\"\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(query: str, top_k: int = 5):\n",
    "\n",
    "    q_vec = vectorizer.transform([_expand_query(query)])\n",
    "\n",
    "    sims = cosine_similarity(q_vec, matrix).ravel()\n",
    "\n",
    "    idxs = sims.argsort()[::-1][:top_k]\n",
    "\n",
    "    return [(int(i), float(sims[i]), kb_chunks[int(i)]) for i in idxs]\n",
    "\n",
    "\n",
    "\n",
    "# --- Prompting ---\n",
    "\n",
    "def build_system_prompt() -> str:\n",
    "\n",
    "    return (\n",
    "\n",
    "        \"You are Joni Eats’ cafe assistant. Be friendly, concise, and accurate. \"\n",
    "\n",
    "        \"Answer only cafe-related questions (menu, hours, location, dietary info, specials, ordering, events). \"\n",
    "\n",
    "        \"If a question is outside scope, briefly decline and steer back to cafe topics. \"\n",
    "\n",
    "        \"If the user asks about vegan/vegetarian and the KB lists such items, do not say we don't have them. \"\n",
    "\n",
    "        \"Cite menu items or policies when relevant. Use bullet points when listing options.\"\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def render_prompt(system_prompt: str, query: str, retrieved_context: list[tuple[int, float, str]], few_shots: str | None = None, dietary_hint: str | None = None):\n",
    "\n",
    "    context_block = \"\\n\\n\".join([f\"[Snippet {i}]\\n{txt}\" for i, _s, txt in retrieved_context])\n",
    "\n",
    "    msgs = [\n",
    "\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "\n",
    "        {\"role\": \"system\", \"content\": \"Context from cafe knowledge base:\\n\" + (context_block or \"(no relevant context)\")},\n",
    "\n",
    "    ]\n",
    "\n",
    "    if dietary_hint:\n",
    "\n",
    "        msgs.append({\"role\": \"system\", \"content\": dietary_hint})\n",
    "\n",
    "    if few_shots and few_shots.strip():\n",
    "\n",
    "        msgs.append({\"role\": \"system\", \"content\": \"Example interactions:\\n\" + few_shots.strip()})\n",
    "\n",
    "    msgs.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    return msgs\n",
    "\n",
    "\n",
    "\n",
    "# --- Groq Client & Model Aliases ---\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "MODEL_ALIASES = {\n",
    "\n",
    "    \"llama3-8b-8192\": \"llama-3.1-8b-instant\",\n",
    "\n",
    "    \"llama3-70b-8192\": \"llama-3.1-70b-versatile\",\n",
    "\n",
    "}\n",
    "\n",
    "def normalize_model(model: str) -> str:\n",
    "\n",
    "    return MODEL_ALIASES.get(model, model)\n",
    "\n",
    "\n",
    "\n",
    "def groq_chat(messages: list[dict], model: str = \"llama-3.1-8b-instant\", temperature: float = 0.2) -> str:\n",
    "\n",
    "    model = normalize_model(model)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "\n",
    "        model=model,\n",
    "\n",
    "        messages=messages,\n",
    "\n",
    "        temperature=temperature,\n",
    "\n",
    "    )\n",
    "\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "# --- Dietary Hint ---\n",
    "\n",
    "def _dietary_hint_for(query: str) -> str | None:\n",
    "\n",
    "    ql = query.lower()\n",
    "\n",
    "    if any(t in ql for t in [\"vegan\", \"plant-based\", \"plant based\", \"veggie\", \"vegetarian\"]):\n",
    "\n",
    "        if diet_index.get(\"vegan\") or diet_index.get(\"vegetarian\"):\n",
    "\n",
    "            items = (diet_index.get(\"vegan\") or [])[:3] + (diet_index.get(\"vegetarian\") or [])[:3]\n",
    "\n",
    "            items = [f\"- {it}\" for it in items[:5]]\n",
    "\n",
    "            return \"Dietary guidance: The menu lists vegan/vegetarian options. Prefer offering them if relevant.\\n\" + \"\\n\".join(items)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# --- Memory & Safety ---\n",
    "\n",
    "MAX_HISTORY = 16\n",
    "\n",
    "PROFANITY_RE = re.compile(r\"\\b(fuck|shit|bitch)\\b\", re.I)\n",
    "\n",
    "\n",
    "\n",
    "def filter_input(text: str) -> str:\n",
    "\n",
    "    if len(text) > 1000:\n",
    "\n",
    "        text = text[:1000] + \"…\"\n",
    "\n",
    "    if PROFANITY_RE.search(text):\n",
    "\n",
    "        text = PROFANITY_RE.sub(\"(redacted)\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def roll_history(history: list[dict], new_user: str, new_assistant: str | None = None):\n",
    "\n",
    "    history.append({\"role\": \"user\", \"content\": new_user})\n",
    "\n",
    "    if new_assistant:\n",
    "\n",
    "        history.append({\"role\": \"assistant\", \"content\": new_assistant})\n",
    "\n",
    "    if len(history) > MAX_HISTORY:\n",
    "\n",
    "        del history[:-MAX_HISTORY]\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "# --- Public API ---\n",
    "\n",
    "def answer_query(query: str, top_k: int = 5, temperature: float = 0.2, model: str = \"llama-3.1-8b-instant\") -> dict:\n",
    "\n",
    "    query = filter_input(query)\n",
    "\n",
    "    hits = retrieve(query, top_k=top_k)\n",
    "\n",
    "    system_p = build_system_prompt()\n",
    "\n",
    "    few_shots = chat_patterns\n",
    "\n",
    "    dietary_hint = _dietary_hint_for(query)\n",
    "\n",
    "    messages = render_prompt(system_p, query, hits, few_shots, dietary_hint=dietary_hint)\n",
    "\n",
    "    try:\n",
    "\n",
    "        text = groq_chat(messages, model=model, temperature=temperature)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        text = \"I'm not sure about that right now. Please ask about our menu, hours, or specials.\"\n",
    "\n",
    "    return {\n",
    "\n",
    "        \"text\": text,\n",
    "\n",
    "        \"retrieved\": hits,\n",
    "\n",
    "        \"messages\": messages,\n",
    "\n",
    "        \"model\": model,\n",
    "\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
