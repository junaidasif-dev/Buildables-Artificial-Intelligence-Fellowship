{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6da588",
   "metadata": {},
   "source": [
    "# **Buckle Up ! We are starting our week 2 roller coaster**\n",
    "\n",
    "In our first week we covered some theoritical concepts and completed our setup so its time we start building!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b55c5",
   "metadata": {},
   "source": [
    "## üìì**Conversational AI Concepts & Model Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7ace2",
   "metadata": {},
   "source": [
    "üéØ By the end of this week, you will:\n",
    "\n",
    "- Understand LLMs, STT, TTS models and their roles.\n",
    "\n",
    "- Know how to connect to LLMs with APIs (Groq as example).\n",
    "\n",
    "- Use Python (requests + JSON) for API interaction.\n",
    "\n",
    "- Start building a basic chatbot with memory and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c5144",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Large Language Models (LLMs) üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03968dc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùó **Question 1**: What is an LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd894fc",
   "metadata": {},
   "source": [
    "üëâ It‚Äôs like a super-smart text predictor that can read, understand, and generate human-like sentences.\n",
    "\n",
    "You give it some words ‚Üí it guesses the next words in a way that makes sense.\n",
    "\n",
    "For example:\n",
    "\n",
    "1) You ask a question ‚Üí it gives you an answer.\n",
    "\n",
    "2) You write a sentence ‚Üí it can complete it.\n",
    "\n",
    "3) You give it a topic ‚Üí it can write an essay, code, or even a story.\n",
    "\n",
    "So, its a type of AI trained on huge amounts of text data to generate or understand text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77076ddc",
   "metadata": {},
   "source": [
    "### Types of LLMs\n",
    "\n",
    "1. Encoder-only models (e.g., BERT)\n",
    "\n",
    "    - Best for understanding text (classification, sentiment analysis, embeddings).\n",
    "\n",
    "    - ‚ùå Not good at generating text.\n",
    "\n",
    "2. Decoder-only models (e.g., GPT, LLaMA, Mistral)\n",
    "\n",
    "    - Best for text generation (chatbots, writing, summarization).\n",
    "\n",
    "    - What we use in chatbots.\n",
    "\n",
    "3. Encoder-decoder models (e.g., T5, BART)\n",
    "\n",
    "    - Good at transforming text (translation, summarization, Q&A)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339099fe",
   "metadata": {},
   "source": [
    "### Must-Knows about LLMs\n",
    "\n",
    "- They don‚Äôt ‚Äúthink‚Äù like humans ‚Üí They predict text based on training.\n",
    "\n",
    "- Garbage in ‚Üí garbage out: Poor prompts = poor answers.\n",
    "\n",
    "- Token limits: Models can only ‚Äúsee‚Äù a certain number of words at a time.\n",
    "\n",
    "- Biases: Trained on internet text ‚Üí may reflect biases/errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b2dd4",
   "metadata": {},
   "source": [
    "### üí° **Quick Questions**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753565a",
   "metadata": {},
   "source": [
    "1. Why might a chatbot built on BERT (encoder-only) struggle to answer open-ended questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31834ad2",
   "metadata": {},
   "source": [
    "A BERT (encoder‚Äëonly) chatbot struggles with open‚Äëended answers because it is designed to produce contextual embeddings for understanding, not to autoregressively generate fluent continuations.\n",
    "\n",
    "Key reasons:\n",
    "- No decoder / generation head: Vanilla BERT is bidirectional; it ‚Äúsees‚Äù both left and right context while training (masked language modeling). It does not learn to emit a long sequence token‚Äëby‚Äëtoken the way decoder models (GPT, LLaMA) do.\n",
    "- Masked objective mismatch: Predicting randomly masked tokens teaches *fill‚Äëin* understanding, not sustained narrative or dialog flow.\n",
    "- Output shape: Typical BERT fine‚Äëtunes to classification (single label), span extraction (start/end indices), or embedding tasks‚Äînot freeform text strings.\n",
    "- Lack of autoregressive memory: Decoder models maintain a growing context and generate the next token conditioned on everything so far; BERT would need extra architecture (e.g., a seq2seq wrapper) to do that.\n",
    "- Generation hacks are clunky: You can sample masks iteratively or attach a lightweight decoder, but quality, coherence, and length control are worse than purpose‚Äëbuilt decoder models.\n",
    "- Conversational dynamics: Open‚Äëended answers require pragmatic choices (length, style, elaboration). BERT provides semantic understanding but not a learned policy for multi‚Äësentence composition.\n",
    "\n",
    "Rule of thumb:\n",
    "- Use encoder (BERT) for understanding / classification / retrieval.\n",
    "- Use decoder (GPT‚Äëstyle) for freeform generation and open‚Äëended chat.\n",
    "- Use encoder‚Äëdecoder (T5/BART) when you need to *transform* one text into another (summarization, translation) with strong conditioning.\n",
    "\n",
    "So: A BERT chatbot ‚Äúknows‚Äù context but lacks the generative machinery to speak freely without additional components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5feffca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Speech-to-Text (STT) üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393abf7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùó **Question 2**: What is STT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54ac00",
   "metadata": {},
   "source": [
    "üëâ listens to your voice and turns it into written text.\n",
    "\n",
    "- Converts **audio ‚Üí text**.\n",
    "- Enables voice input for conversational AI.\n",
    "- Think of it as the **ears** of the chatbot.\n",
    "\n",
    "**Popular STT Models**:\n",
    "\n",
    "1) **Whisper (OpenAI)** ‚Äì strong at multilingual speech recognition.\n",
    "2) **Google Speech-to-Text API** ‚Äì widely used, real-time transcription.\n",
    "3) **Vosk** ‚Äì lightweight, offline speech recognition.\n",
    "\n",
    "**Common Usages**\n",
    "\n",
    "1) Voice assistants (Alexa, Siri, Google Assistant).\n",
    "2) Automated captions in meetings or lectures.\n",
    "3) Voice-enabled customer support.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99714c",
   "metadata": {},
   "source": [
    "### Must-Knows about STT\n",
    "\n",
    "- Accuracy depends on **noise, accents, clarity of speech**.\n",
    "\n",
    "- Some models need **internet connection** (API-based), others run **offline**.\n",
    "\n",
    "- Preprocessing audio (noise reduction) improves results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23bf9a",
   "metadata": {},
   "source": [
    "### üí° **Quick Questions**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d8a82",
   "metadata": {},
   "source": [
    "2. Why do you think meeting transcription apps like Zoom or Google Meet struggle when multiple people talk at once?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679a820",
   "metadata": {},
   "source": [
    "Because current speech recognition pipelines assume (mostly) one dominant speaker at a time; overlapping speech breaks their assumptions and signal quality.\n",
    "\n",
    "Main challenges:\n",
    "- Overlapping waveforms: Two voices mix additively ‚Üí the model receives a single tangled acoustic signal; standard STT expects one source.\n",
    "- Voice separation (source separation) is hard: Need to unmix (‚Äúwho said what‚Äù) before transcription; errors cascade downstream.\n",
    "- Speaker diarization limits: Diarization models detect speaker turns; rapid interjections or simultaneous talk confuses boundary detection ‚Üí timestamps & labels drift.\n",
    "- Acoustic masking: Louder speaker masks quieter one; weaker segments fall below model confidence thresholds ‚Üí omissions.\n",
    "- Accents & differing timbre: When overlapped, spectral features blur ‚Üí phoneme recognition accuracy drops.\n",
    "- Latency constraints: Real-time meeting apps can‚Äôt run heavy separation (e.g., advanced neural source separation + language modeling) without adding lag.\n",
    "- Noise + echo: Conference mics capture room reverberation; overlapping speech + echo increases ambiguity.\n",
    "- Language model context corruption: Partial words from two speakers interleave, producing sequences not seen in training ‚Üí more decoding errors.\n",
    "\n",
    "Mitigations (partial):\n",
    "- Multi-channel input: Per-participant audio streams (headsets) instead of a single room mic.\n",
    "- Neural source separation (e.g., ConvTasNet, Demucs variants) prior to ASR; still imperfect for >2 speakers or heavy overlap.\n",
    "- Better diarization with embeddings (x-vectors) + overlap-aware models.\n",
    "- Turn-taking UX cues: Encourage minimal overlap (push-to-talk, visual indicators).\n",
    "- Post-processing alignment: Combine partial hypotheses and reconcile with language context.\n",
    "\n",
    "Rule of thumb: Accurate transcription = clean single-speaker segments. As simultaneity increases, word error rate rises sharply unless you invest in separation + diarization pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c851b3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Text-to-Speech (TTS) üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650b62d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùó **Question 3**: What is TTS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb26f02",
   "metadata": {},
   "source": [
    "üëâ takes written text and speaks it out loud in a human-like voice.\n",
    "\n",
    "- Converts **text ‚Üí audio (speech)**.\n",
    "- Think of it as the **mouth** of the chatbot.\n",
    "- Makes AI ‚Äúspeak‚Äù naturally.\n",
    "\n",
    "**Popular TTS Models**:\n",
    "\n",
    "1) **Google TTS** ‚Äì supports many languages and voices.\n",
    "2) **Amazon Polly** ‚Äì lifelike voice synthesis with customization.\n",
    "3) **ElevenLabs** ‚Äì cutting-edge, realistic voice cloning.\n",
    "\n",
    "**Common Usages**\n",
    "\n",
    "1) Screen readers for visually impaired users.\n",
    "2) AI chatbots with voice output.\n",
    "3) Audiobooks or podcast generation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb2471",
   "metadata": {},
   "source": [
    "### Must-Knows about TTS\n",
    "\n",
    "- Some voices sound robotic; others use **neural TTS** for natural tones.\n",
    "\n",
    "- Latency matters ‚Üí If too slow, conversation feels unnatural.\n",
    "\n",
    "- Some TTS services allow **custom voices**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49cb51",
   "metadata": {},
   "source": [
    "### üí° **Quick Questions**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f3eb2",
   "metadata": {},
   "source": [
    "3. If you were designing a voice-based AI tutor, what qualities would you want in its TTS voice (tone, speed, clarity, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7081b9",
   "metadata": {},
   "source": [
    "For a voice‚Äëbased AI tutor, the TTS voice should balance clarity, warmth, and adaptability:\n",
    "\n",
    "Core qualities:\n",
    "- Clear articulation: Distinct phonemes so learners at different proficiency levels can follow.\n",
    "- Neutral accent (or selectable): Minimizes regional bias; offer optional localized variants.\n",
    "- Moderate pace with adaptive speed: ~150‚Äì170 wpm baseline, slow down for definitions, speed up for review.\n",
    "- Warm, encouraging tone: Friendly but not overly playful; slight prosody variation to avoid monotony.\n",
    "- Consistent volume & dynamic range: Enough variation for emphasis without large loudness jumps.\n",
    "- Proper pausing: Short pauses after clauses; longer pauses after key concepts or questions to invite reflection.\n",
    "- Emphasis & prosody control: Highlight keywords (slight pitch rise) to aid memory.\n",
    "- Low latency: Fast start (<300ms) so dialog feels natural.\n",
    "\n",
    "Adaptive behaviors:\n",
    "- Context-aware pacing: Detects code, formulas, or lists ‚Üí slows and inserts separators.\n",
    "- Emotional calibration: Calmer tone for corrections; enthusiastic tone for achievements.\n",
    "- Personalization: Adjusts speed, pitch, or energy based on user preference or performance (e.g., slower after repeated mistakes).\n",
    "\n",
    "Accessibility considerations:\n",
    "- High intelligibility in noisy environments (robust phoneme modeling).\n",
    "- Optional simplified mode (slower, more explicit punctuation verbalization for screen reader synergy).\n",
    "\n",
    "Avoid:\n",
    "- Excessive expressiveness (can distract from learning).\n",
    "- Robotic flatness (reduces engagement & retention).\n",
    "- Overly gendered stereotypes‚Äîoffer a few balanced, professional voices.\n",
    "\n",
    "Extra features (nice to have):\n",
    "- Inline spelling mode (for difficult terms on request).\n",
    "- Pronunciation contrast (‚Äúcolor‚Äù vs ‚Äúcaller‚Äù in minimal pairs) when teaching language nuances.\n",
    "\n",
    "Rule of thumb: Start neutral + clear, then let the learner dial in speed, warmth, and detail level as they progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042c582",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Using APIs for LLMs with Groq üåü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7889d8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded. Initializing client...\n",
      "Conversational AI, also known as conversational systems or conversational interfaces, is a subset of artificial intelligence (AI) that enables computers to interact with humans in a more natural and intuitive way through conversations. \n",
      "\n",
      "These systems can understand and respond to spoken or written language in a way that simulates human-like conversation. Conversational AI can include various applications, such as:\n",
      "\n",
      "1. Chatbots: Computer programs designed to converse with humans through text-based interfaces.\n",
      "2. Virtual assistants: AI-powered programs that can understand voice commands and perform specific tasks, such as Amazon's Alexa or Google Assistant.\n",
      "3. Voice assistants: Similar to virtual assistants but focus on spoken language, and can include systems like Siri or Cortana.\n",
      "4. Dialogue systems: Software designed to engage in conversation with humans, often using natural language processing (NLP) and machine learning (ML) algorithms to understand the context and respond accordingly.\n",
      "\n",
      "Key features of conversational AI include:\n",
      "\n",
      "1. Natural Language Understanding (NLU): The ability to comprehend and interpret human language.\n",
      "2. Natural Language Generation (NLG): The ability to generate human-like responses to user input.\n",
      "3. Contextual understanding: The ability to keep track of the conversation's flow and context to provide relevant responses.\n",
      "4. Learning and improvement: The ability to learn from user interactions and improve their overall performance.\n",
      "\n",
      "Conversational AI has various applications in different domains, including customer service, healthcare, education, and entertainment. It can help improve user experiences, enhance customer service, and increase the efficiency of many processes.\n",
      "\n",
      "How would you like to proceed? Would you like to know more about specific applications or technical aspects of conversational AI?\n",
      "Conversational AI, also known as conversational systems or conversational interfaces, is a subset of artificial intelligence (AI) that enables computers to interact with humans in a more natural and intuitive way through conversations. \n",
      "\n",
      "These systems can understand and respond to spoken or written language in a way that simulates human-like conversation. Conversational AI can include various applications, such as:\n",
      "\n",
      "1. Chatbots: Computer programs designed to converse with humans through text-based interfaces.\n",
      "2. Virtual assistants: AI-powered programs that can understand voice commands and perform specific tasks, such as Amazon's Alexa or Google Assistant.\n",
      "3. Voice assistants: Similar to virtual assistants but focus on spoken language, and can include systems like Siri or Cortana.\n",
      "4. Dialogue systems: Software designed to engage in conversation with humans, often using natural language processing (NLP) and machine learning (ML) algorithms to understand the context and respond accordingly.\n",
      "\n",
      "Key features of conversational AI include:\n",
      "\n",
      "1. Natural Language Understanding (NLU): The ability to comprehend and interpret human language.\n",
      "2. Natural Language Generation (NLG): The ability to generate human-like responses to user input.\n",
      "3. Contextual understanding: The ability to keep track of the conversation's flow and context to provide relevant responses.\n",
      "4. Learning and improvement: The ability to learn from user interactions and improve their overall performance.\n",
      "\n",
      "Conversational AI has various applications in different domains, including customer service, healthcare, education, and entertainment. It can help improve user experiences, enhance customer service, and increase the efficiency of many processes.\n",
      "\n",
      "How would you like to proceed? Would you like to know more about specific applications or technical aspects of conversational AI?\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not set. Add it to your .env file or export as an environment variable.\")\n",
    "\n",
    "# Simple sanity check so we know code executed up to here without indentation errors\n",
    "print(\"Environment loaded. Initializing client...\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "         \n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful tutor.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello! What is conversational AI?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c20eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Assignments üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc2ce6",
   "metadata": {},
   "source": [
    "### üìù Assignment 1: LLM Understanding\n",
    "\n",
    "* Write a short note (3‚Äì4 sentences) explaining the difference between **encoder-only, decoder-only, and encoder-decoder LLMs**.\n",
    "* Give one example usage of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812f093",
   "metadata": {},
   "source": [
    "**Answer:** Encoder-only models (e.g., BERT) read the whole sentence bidirectionally to produce rich embeddings for understanding tasks like sentiment or intent classification‚Äîgreat at ‚Äúreading,‚Äù not at long freeform ‚Äúspeaking.‚Äù Decoder-only models (e.g., GPT, LLaMA) generate text left‚Äëto‚Äëright, making them ideal for open‚Äëended tasks like chatbots, story or code generation. Encoder‚Äëdecoder (seq2seq) models (e.g., T5, BART) first encode the input, then a decoder generates a new sequence conditioned on that representation‚Äîperfect for transforming text (translation, summarization, Q&A). Examples: encoder-only ‚Üí spam detection; decoder-only ‚Üí conversational assistant; encoder‚Äëdecoder ‚Üí English‚ÜíSpanish translation or abstractive news summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370084b8",
   "metadata": {},
   "source": [
    "### üìù Assignment 2: STT/TTS Exploration\n",
    "\n",
    "* Find **one STT model** and **one TTS model** (other than Whisper/Google).\n",
    "* Write down:\n",
    "\n",
    "  * What it does.\n",
    "  * One possible application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dca76d",
   "metadata": {},
   "source": [
    "**Answer:** STT model ‚Äì wav2vec 2.0 (Facebook AI): learns speech representations self‚Äësupervised from raw audio, then fine‚Äëtuned for transcription; strong accuracy with less labeled data. Application: real‚Äëtime captioning in low‚Äëresource languages after fine‚Äëtuning on a small local dataset. TTS model ‚Äì Tacotron 2: sequence‚Äëto‚Äësequence model that converts text ‚Üí mel spectrograms then uses a vocoder (e.g., WaveGlow / HiFi-GAN) to synthesize natural speech. Application: generating consistent branded voice lines for an educational app without hiring voice actors for every update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84824b65",
   "metadata": {},
   "source": [
    "### üìù Assignment 3: Build a Chatbot with Memory\n",
    "\n",
    "* Write a Python program that:\n",
    "\n",
    "  * Takes user input in a loop.\n",
    "  * Sends it to Groq API.\n",
    "  * Stores the last 5 messages in memory.\n",
    "  * Ends when user types `\"quit\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef3132",
   "metadata": {},
   "source": [
    "### üìù Assignment 4: Preprocessing Function\n",
    "\n",
    "* Write a function to clean user input:\n",
    "\n",
    "  * Lowercase text.\n",
    "  * Remove punctuation.\n",
    "  * Strip extra spaces.\n",
    "\n",
    "Test with: `\"  HELLo!!!  How ARE you?? \"`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53027998",
   "metadata": {},
   "source": [
    "### üìù Assignment 5: Text Preprocessing\n",
    "\n",
    "* Write a function that:\n",
    "\n",
    "    * Converts text to lowercase.\n",
    "    * Removes punctuation & numbers.\n",
    "    * Removes stopwords (`the, is, and...`).\n",
    "    * Applies stemming or lemmatization.\n",
    "    * Removes words shorter than 3 characters.\n",
    "    * Keeps only nouns, verbs, and adjectives (using POS tagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68c035",
   "metadata": {},
   "source": [
    "### üìù Assignment 6: Reflection\n",
    "\n",
    "* Answer in 2‚Äì3 sentences:\n",
    "\n",
    "    * Why is context memory important in chatbots?\n",
    "    * Why should beginners always check **API limits and pricing**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa02581",
   "metadata": {},
   "source": [
    "**Context memory importance in chatbots**  \n",
    "Context memory lets a chatbot stay coherent and useful‚Äîit can resolve pronouns (\"that\", \"it\"), avoid repeating earlier questions, and personalize guidance based on prior user intent or preferences.   \n",
    "**API limits and pricing**  \n",
    "Beginners must check API limits and pricing early to prevent surprise bills and throttling; understanding cost per token/request shapes prompt length, batching, retries, and helps design an efficient, sustainable workflow from day one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b787de4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Hints:**\n",
    "\n",
    "1) Stemming:\n",
    "    - Cuts off word endings to get the ‚Äúroot.‚Äù\n",
    "    - Very mechanical ‚Üí may produce non-real words.\n",
    "    - Example:\n",
    "        - \"studies\" ‚Üí \"studi\"\n",
    "        - \"running\" ‚Üí \"run\"\n",
    "\n",
    "2) Lemmatization:\n",
    "    - Smarter ‚Üí uses vocabulary + grammar rules.\n",
    "    - Always gives a real word (the **lemma**).\n",
    "    - Example:\n",
    "        - \"studies\" ‚Üí \"study\"\n",
    "        - \"running\" ‚Üí \"run\"\n",
    "\n",
    "3) Part-of-Speech (POS) tagging means labeling each word in a sentence with its grammatical role ‚Äî like **noun, verb, adjective, adverb, pronoun, etc.**\n",
    "\n",
    "    - Example:\n",
    "        - Sentence ‚Üí *‚ÄúThe cat is sleeping on the mat.‚Äù*\n",
    "\n",
    "    - POS tags ‚Üí\n",
    "        - The ‚Üí Determiner (DT)\n",
    "        - cat ‚Üí Noun (NN)\n",
    "        - is ‚Üí Verb (VBZ)\n",
    "        - sleeping ‚Üí Verb (VBG)\n",
    "        - on ‚Üí Preposition (IN)\n",
    "        - the ‚Üí Determiner (DT)\n",
    "        - mat ‚Üí Noun (NN)\n",
    "\n",
    "    - **In short:** POS tagging helps machines understand **how words function in a sentence**, which is useful in NLP tasks like machine translation, text classification, and question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec98bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ Recap\n",
    "\n",
    "This week you learned:\n",
    "\n",
    "* **LLMs**: Types, uses, must-knows.\n",
    "* **STT & TTS**: How they connect with LLMs.\n",
    "* **APIs**: Connecting to LLMs with Groq.\n",
    "* Built your first chatbot foundation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
