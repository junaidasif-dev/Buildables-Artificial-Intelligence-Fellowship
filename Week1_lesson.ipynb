{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66f0500",
   "metadata": {},
   "source": [
    "# üöÄ **Introduction to APIs and LLMs with Python**\n",
    "\n",
    "Welcome! üëã  \n",
    "In this notebook, we will learn the basics of AI (Artificial Intelligence), LLMs (Large Language Models), APIs (Application Programming Interfaces).\n",
    "\n",
    "By the end of this notebook, you will:  \n",
    "- What is AI.  \n",
    "- What are LLM.  \n",
    "- What are API.\n",
    "- Jupyter notebook or Google colab.\n",
    "- Save and share your notebook using GitHub.  \n",
    "\n",
    "Let‚Äôs begin üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf80d1c",
   "metadata": {},
   "source": [
    "# üåü But first start with some basic recap!üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83150a22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùó **Question 1**: What is Artificial Intelligence (AI)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b74016",
   "metadata": {},
   "source": [
    "üëâ Artificial Intelligence (AI) means teaching computers to think and act like humans.\n",
    "\n",
    "Just like we use our brain to learn, solve problems, and make decisions, AI helps computers do the same.\n",
    "\n",
    "For example:\n",
    "\n",
    "1) When you talk to Siri or Alexa and they answer you ‚Üí that‚Äôs AI.\n",
    "\n",
    "2) When YouTube shows you videos you may like ‚Üí that‚Äôs AI.\n",
    "\n",
    "3) When a self-driving car follows the road ‚Üí that‚Äôs AI.\n",
    "\n",
    "So, AI is like giving a computer a mini brain to understand, learn, and decide. OR in simple words AI is just automation...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa29b08",
   "metadata": {},
   "source": [
    "### üí° **Quick Questions**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202be2da",
   "metadata": {},
   "source": [
    "1. Do yo think AI should be defined by how it is built, or by what it can do? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fe8e7",
   "metadata": {},
   "source": [
    "AI is best defined primarily by what it can do (the intelligent behaviors it exhibits), with how it‚Äôs built used secondarily to qualify its limits.\n",
    "\n",
    "- Capabilities (reasoning, learning, perception, language use) are observable and comparable across very different architectures.\n",
    "- Focusing only on how it‚Äôs built excludes future or alternative implementations (biological, symbolic, hybrid).\n",
    "- A capability-based definition aligns with why we care: solving tasks that normally need human intelligence.\n",
    "- Build details still matter for safety, transparency, and accountability‚Äîso mechanism refines, not anchors, the definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798c03b",
   "metadata": {},
   "source": [
    "2. Today's AI is mostly narrow AI (like bots, recommendation systems, etc). What do you think true 'general AI' would look like - and how close are we to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7523aa",
   "metadata": {},
   "source": [
    "A true general AI (AGI) would:\n",
    "- Transfer learning: apply knowledge from one domain to a very different one with little new data.\n",
    "- Robust reasoning: handle ambiguity, plan long‚Äëterm, and revise beliefs with evidence.\n",
    "- Grounded understanding: link symbols to real/world models (cause‚Äìeffect, physical & social context).\n",
    "- Autonomy & goal management: set, prioritize, and decompose goals while staying aligned with constraints.\n",
    "- Continual learning: improve online without catastrophic forgetting.\n",
    "- Reliability: remain safe, honest, and consistent under distribution shifts.\n",
    "\n",
    "Where we are now:\n",
    "- Current LLMs = broad linguistic pattern engines, not truly general (shallow world models, brittle with novel tasks, no persistent self-driven learning).\n",
    "- Missing pieces: causal reasoning, stable memory, embodiment/sensorimotor grounding, verifiable alignment, efficient learning (humans learn from far less data).\n",
    "- Progress is fast in scaling + tool use + multi‚Äëmodal integration; convergence ‚â† emergence of full generality.\n",
    "- Timelines are uncertain; expert views span ‚Äúmaybe this decade‚Äù to ‚Äúmany decades.‚Äù No solid evidence it is imminent.\n",
    "\n",
    "Treat today‚Äôs systems as powerful narrow tools with expanding breadth, not yet generally intelligent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5587b81",
   "metadata": {},
   "source": [
    "3. Can AI ‚Äòunderstand‚Äô things the way humans do, or is it just simulating understanding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1aa0f",
   "metadata": {},
   "source": [
    "Current AI *appears* to understand because it produces the right words, but it does this by recognizing and recombining statistical patterns learned from huge amounts of data.\n",
    "\n",
    "- No lived experience: It hasn‚Äôt seen, felt, or interacted with the real world directly (no embodied grounding).\n",
    "- Pattern prediction engine: It predicts the next likely token; success can *look* like comprehension.\n",
    "- Lacks true concepts: Internal vectors capture correlations, not human-style meaning tied to sensation & goals.\n",
    "- No self-model or awareness: It doesn‚Äôt ‚Äúknow that it knows‚Äù; no feelings, intentions, or curiosity.\n",
    "- Partial structure: Still, the representations encode useful relationships (syntax, facts, analogies) ‚Üí a kind of ‚Äúfunctional‚Äù or ‚Äúas-if‚Äù understanding.\n",
    "\n",
    "Treat outputs as plausible, not guaranteed. It simulates understanding well enough to help‚Äîhumans must validate important answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa230e0f",
   "metadata": {},
   "source": [
    "4. If an AI system makes a decision that harms someone, who should be responsible: the AI, the developer, or the user?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3572d",
   "metadata": {},
   "source": [
    "The AI itself isn‚Äôt a moral agent‚Äîit‚Äôs a tool. Responsibility sits with people and organizations around it. A good way to think about it is layered accountability:\n",
    "\n",
    "1. Creators / Developers\n",
    "- Must design, test, and document limits, reduce bias, add safety guards.\n",
    "- If the harm comes from sloppy design or ignored warnings ‚Üí they hold a big share of responsibility.\n",
    "\n",
    "2. Deployers / Companies / Product Owners\n",
    "- Decide how the model is integrated, what data it gets, what decisions it‚Äôs allowed to make.\n",
    "- Must provide monitoring, human override, clear user guidance, logging.\n",
    "\n",
    "3. Users / Operators\n",
    "- Should use it as intended, not bypass safeguards, and review critical outputs (e.g., medical, legal, hiring).\n",
    "- If they misuse it against guidance ‚Üí responsibility shifts toward them.\n",
    "\n",
    "4. Regulators / Standards Bodies\n",
    "- Set baseline rules (privacy, safety, fairness). If rules are ignored, liability increases.\n",
    "\n",
    "Why not ‚Äúthe AI‚Äù? It has no intent, awareness, or capacity to choose differently‚Äîso blaming it doesn‚Äôt fix process failures.\n",
    "\n",
    "Simple rule of thumb:\n",
    "- Design flaw ‚Üí developers.\n",
    "- Unsafe deployment or lack of oversight ‚Üí deployer/company.\n",
    "- Clear misuse after proper warnings ‚Üí user.\n",
    "\n",
    "Best practice: Shared responsibility + audit trails + human-in-the-loop for high‚Äëstakes domains (healthcare, finance, justice). That‚Äôs how we keep trust and reduce harm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18502d3",
   "metadata": {},
   "source": [
    "5. Should AI be allowed to make decisions in areas like healthcare, hiring, or law? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a20a22b",
   "metadata": {},
   "source": [
    "AI can assist, but final high‚Äëstakes decisions in healthcare, hiring, and law should stay with humans‚Äîat least until we have stronger guarantees of fairness, transparency, and accountability.\n",
    "\n",
    "Why allow AI involvement?\n",
    "- Speed & scale: Triage symptoms, scan medical images, sift thousands of resumes, flag legal document patterns.\n",
    "- Consistency: Doesn‚Äôt get tired; applies the same checklist every time.\n",
    "- Discovery: Can surface subtle correlations humans might miss.\n",
    "\n",
    "Why not fully hand over decisions?\n",
    "- Bias risk: Training data can encode historic discrimination (e.g., against certain demographics).\n",
    "- Opaqueness: Deep models‚Äô reasoning is hard to explain to patients, candidates, or courts.\n",
    "- Accountability gap: Who do you blame or appeal to if the model is wrong?\n",
    "- Context & ethics: Edge cases need judgment, empathy, and value trade‚Äëoffs AI doesn‚Äôt truly grasp.\n",
    "- Distribution shift: Real-world changes (new diseases, job market shifts, novel legal precedents) can silently degrade accuracy.\n",
    "\n",
    "Model to use:\n",
    "1. AI suggests / ranks / highlights.\n",
    "2. Human reviews, questions, overrides.\n",
    "3. System logs rationale and inputs for audit.\n",
    "\n",
    "Minimum safeguards before deployment:\n",
    "- Bias & performance audits across subgroups.\n",
    "- Clear scope limits (where not to use it).\n",
    "- Explainability or at least traceable factors.\n",
    "- Appeal process for affected people.\n",
    "- Continuous monitoring & retraining triggers.\n",
    "\n",
    "Rule of thumb:\n",
    "- Automate low-risk, repetitive filtering (assistive).\n",
    "- Require human confirmation for medium/high impact (supportive).\n",
    "- For irreversible, life‚Äëaltering outcomes (surgery plan, legal sentencing, hiring rejection): AI = advisor, not decider.\n",
    "\n",
    "Use AI to augment professional judgment, not replace it‚Äîuntil governance, robustness, and ethical frameworks mature further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c001a",
   "metadata": {},
   "source": [
    "6. If an AI composes music or paints artwork, who owns the rights ‚Äî the AI, the programmer, or nobody?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb153e0",
   "metadata": {},
   "source": [
    "The AI doesn‚Äôt own anything (it‚Äôs not a legal person). Rights usually go to the human(s) who supply meaningful creative input‚Äîor sometimes nobody if it‚Äôs fully machine‚Äëgenerated without human authorship.\n",
    "\n",
    "How it breaks down:\n",
    "- AI system itself: No legal personality ‚Üí cannot hold copyright.\n",
    "- Programmer / model creator: Owns the model + code weights, not every output others generate with it (unless license terms say otherwise).\n",
    "- End user / prompter: May gain rights if their contribution rises to *human authorship* (selection, arrangement, editing, iterative steering with judgment).\n",
    "- Purely autonomous output (no real human creative choices): In some places (e.g., current US guidance) ‚Üí not protectable; effectively public domain.\n",
    "\n",
    "Jurisdiction snapshots (simplified):\n",
    "- United States: Must show human authorship. Pure AI output alone is not registrable.\n",
    "- EU: Emphasis on human creative control; purely automated likely unprotected.\n",
    "- UK: Special clause: for ‚Äúcomputer-generated‚Äù works, the person making the arrangements may get rights (debated, may evolve).\n",
    "- Others: Still forming policy; trend toward requiring human contribution.\n",
    "\n",
    "Licenses & terms matter:\n",
    "- Some AI platforms claim a license to use your prompts/outputs for improvement.\n",
    "- Open model licenses may add conditions (attribution, restrictions, etc.).\n",
    "\n",
    "Practical rule of thumb:\n",
    "1. Substantial human guidance/editing ‚Üí stronger claim.\n",
    "2. One raw prompt, no shaping ‚Üí weak or no protection.\n",
    "3. Unlicensed training data issues can create downstream risk even if output is new.\n",
    "\n",
    "Best practice:\n",
    "- Keep a brief log of prompts/edits.\n",
    "- Note: ‚ÄúAI-assisted‚Äîfinal curation by [Name].‚Äù\n",
    "- Review service/model terms before commercial use.\n",
    "\n",
    "Humans own; AI doesn‚Äôt. More human creativity and transformation = stronger claim. Fully automatic = often unprotectable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad6c92",
   "metadata": {},
   "source": [
    "7. Which jobs do you think AI will change the most in the next 10 years, and which jobs are the hardest for AI to replace?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ed699",
   "metadata": {},
   "source": [
    "Most impacted (shift in how work is done, not instant disappearance):\n",
    "- Routine knowledge work: report drafting, basic marketing copy, summarizing meetings.\n",
    "- Customer support & help desks: first‚Äëline triage via chat/voice assistants; humans escalate complex/emotional cases.\n",
    "- Data-heavy analysis: junior analyst tasks (cleaning, pattern spotting, dashboard generation) increasingly automated.\n",
    "- Coding assistance: boilerplate, refactors, test scaffolding‚Äîentry-level dev tasks shrink; focus shifts to architecture/integration.\n",
    "- Media production: image/video concepting, storyboard drafts, localization, subtitles, simple edits.\n",
    "- Administrative/operations: scheduling, invoice matching, compliance form filling, document classification.\n",
    "- Translation & transcription: bulk work becomes near‚Äëreal‚Äëtime; specialists focus on nuance, legal, literary tone.\n",
    "\n",
    "Hardest to replace (deep human judgment, physical dexterity in messy environments, trust, or complex social nuance):\n",
    "- Early childhood education & special needs care: emotional attunement, adaptive play, trust building.\n",
    "- Skilled trades in unstructured settings: electricians, plumbers, carpenters (variable layouts, tactile problem solving).\n",
    "- Healthcare roles with bedside interaction: nurses, general practitioners, therapists (empathy + multi-sensory assessment).\n",
    "- Leadership & high-stakes negotiation: aligning incentives, reading subtle social signals, moral accountability.\n",
    "- Creative direction & original concept creation: deciding ‚Äúwhy this matters‚Äù and setting aesthetic/cultural context.\n",
    "- Scientific research design: framing novel hypotheses, cross-domain leaps, interpreting ambiguous results.\n",
    "- Social work & counseling: complex human context, ethics, crisis judgment.\n",
    "\n",
    "Why some tasks are easier to automate:\n",
    "- Digital + repeatable + high volume + clear success metric.\n",
    "- Can be decomposed into prediction + retrieval + formatting steps.\n",
    "\n",
    "Why others resist:\n",
    "- Need for embodied interaction.\n",
    "- Open-ended goals with shifting constraints.\n",
    "- Moral, cultural, or emotional stakes.\n",
    "- Sparse data / rare edge cases where intuition matters.\n",
    "\n",
    "Pattern to watch: Jobs unbundle into micro‚Äëtasks. AI absorbs the most repetitive layers; remaining human work becomes higher-context, cross-disciplinary, or relationship-centered. Careers adapt‚Äînot vanish‚Äîif workers learn to:\n",
    "1. Orchestrate AI tools (prompting, chaining, evaluating).\n",
    "2. Validate and audit outputs (fact-check, bias check, safety).\n",
    "3. Integrate domain knowledge + ethics + system thinking.\n",
    "4. Communicate decisions clearly to stakeholders.\n",
    "\n",
    "Invest in skills that compound with AI (problem framing, systems design, empathy, stewardship of data and models) rather than the parts AI is rapidly commoditizing (generic drafting, rote formatting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227be13c",
   "metadata": {},
   "source": [
    "8. Would you trust an AI to drive your car, perform surgery, or grade your exams? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9be36b",
   "metadata": {},
   "source": [
    "Trust depends on risk, maturity of the tech, and human oversight.\n",
    "\n",
    "Driving (autonomous cars):\n",
    "- Trust partly, in constrained, well-mapped conditions (highways, good weather). Systems already reduce certain human errors (fatigue, distraction).\n",
    "- Don‚Äôt fully trust in edge cases: snow, construction zones, unpredictable pedestrians. Require driver readiness + clear handover alerts.\n",
    "- Key need: transparent disengagement logs, continuous monitoring, conservative fallback behavior.\n",
    "\n",
    "Surgery (robotic / AI-assisted):\n",
    "- Trust as an assistant (image segmentation, instrument stabilization, anomaly flagging) because precision + reduced tremor are real benefits.\n",
    "- Don‚Äôt grant full autonomous control for critical decisions like unexpected bleeding management or ethical trade‚Äëoffs. Human surgeon retains command.\n",
    "- Need: validated clinical trials, post‚Äëoperative outcome audits, explainable recommendations.\n",
    "\n",
    "Grading exams / assessments:\n",
    "- Trust for objective, structured items (multiple choice, spelling, basic coding test cases) to speed turnaround and consistency.\n",
    "- Be cautious on essays, creativity, nuanced reasoning‚Äîrisk of reinforcing bias, misinterpreting originality, penalizing non-standard but valid thinking.\n",
    "- Use AI for first pass + rubric alignment + plagiarism anomaly detection; teacher reviews samples/flags.\n",
    "\n",
    "General principles before trusting:\n",
    "1. Proven reliability on representative data (not just benchmarks).\n",
    "2. Clear failure modes + safe fallbacks.\n",
    "3. Audit trail: what inputs led to what output.\n",
    "4. Human override that is practical (time + interface).\n",
    "5. Ongoing monitoring for drift and bias.\n",
    "\n",
    "Rule of thumb:\n",
    "- High stakes + irreversibility (surgery decisions) ‚Üí AI supports.\n",
    "- Medium stakes + recoverable errors (grading drafts) ‚Üí AI drafts, human curates.\n",
    "- Variable stakes + dynamic environment (driving) ‚Üí shared control with escalating autonomy only where evidence is strongest.\n",
    "\n",
    "So: partial, conditional trust‚Äînever blind delegation. AI augments; humans stay accountable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### ‚ùó **Question 2**: What are LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c4ce8",
   "metadata": {},
   "source": [
    "üëâ LLMs means Large Language Models.\n",
    "\n",
    "They are special computer programs that can read, understand, and write human language (like English, Urdu, or any other).\n",
    "\n",
    "üß† How do they process and generate text?\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "1) Learning stage üìö\n",
    "\n",
    "    - The LLM reads a lot of books, websites, stories, and articles.\n",
    "\n",
    "    - From this, it learns how words are used and which words usually come after each other.\n",
    "\n",
    "2) Understanding stage üëÇ\n",
    "\n",
    "    - When you ask a question (like ‚ÄúWhat is the sky‚Äôs color?‚Äù), it looks at the words carefully.\n",
    "\n",
    "3) Writing stage ‚úçÔ∏è\n",
    "\n",
    "    - It then guesses the next word step by step.\n",
    "\n",
    "    - Example: If you say, ‚ÄúThe sky is‚Ä¶‚Äù\n",
    "    ‚Üí It has seen many times that the next word is ‚Äúblue.‚Äù\n",
    "\n",
    "    - So it answers: ‚ÄúThe sky is blue.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfb9536",
   "metadata": {},
   "source": [
    "üåà Easy way to remember\n",
    "\n",
    "LLM is like a super-smart parrot ü¶ú:\n",
    "\n",
    "- It has read millions of books.\n",
    "\n",
    "- When you talk to it, it replies by picking the best words it learned before.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a28289",
   "metadata": {},
   "source": [
    "### üí° **Quick Questions** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09b3f4",
   "metadata": {},
   "source": [
    "1. What is one task that LLMs perform surprisingly well, and one task where they still struggle ‚Äî and why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a896b6",
   "metadata": {},
   "source": [
    "Strong example (does well):\n",
    "- Rapid summarization of long mixed-topic text into a clear outline. LLMs absorb broad linguistic patterns and can compress salient points because summarizing = predicting likely high-level abstractions seen across training data.\n",
    "\n",
    "Weak example (still struggles):\n",
    "- Reliable step-by-step factual reasoning with multiple dependent calculations (e.g., multi-part math word problems or keeping consistent numbers across several paragraphs). Errors happen because token-by-token prediction can drift; there‚Äôs no guaranteed internal symbolic state enforcing arithmetic or logical consistency.\n",
    "\n",
    "Why this contrast:\n",
    "- Strength: Pattern synthesis over large corpora ‚Üí excels at style, paraphrase, summarization, translation, analogy.\n",
    "- Weakness: Lack of grounded world model + limited working memory tooling ‚Üí brittle on tasks needing strict intermediate verification.\n",
    "\n",
    "How to mitigate weaknesses:\n",
    "- Chain-of-thought prompting + explicit steps.\n",
    "- External tools (calculators, code execution) for math/logic.\n",
    "- Ask for verification or alternative answer check.\n",
    "\n",
    "Great at linguistic compression and style transfer; weaker at precise, stateful, multi-step reasoning without tool support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57a758",
   "metadata": {},
   "source": [
    "2. If an LLM gives a very confident but wrong answer, should we call that a ‚Äòmistake‚Äô like in humans, or something else? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c9fa5",
   "metadata": {},
   "source": [
    "If an LLM gives a very confident but wrong answer, I‚Äôd call it a hallucination or misprediction‚Äînot a human‚Äëstyle ‚Äúmistake.‚Äù\n",
    "\n",
    "Why it feels like a mistake:\n",
    "- Fluency + firm tone signal confidence to us (we map that to a human having a justified belief).\n",
    "\n",
    "Why it‚Äôs different from a human mistake:\n",
    "- No belief state: It isn‚Äôt holding propositions as ‚Äútrue‚Äù; it‚Äôs just predicting the next token.\n",
    "- Objective mismatch: Trained to produce *plausible* continuations, not to guarantee factual accuracy.\n",
    "- No awareness or regret: It can‚Äôt notice it‚Äôs wrong unless we prompt it to re‚Äëevaluate with a different instruction.\n",
    "- Calibration gap: Probability assigned internally to tokens isn‚Äôt a reliable indicator of factual truth for users.\n",
    "\n",
    "Why hallucinations happen:\n",
    "- Pattern completion over missing facts (it fills gaps with statistically likely fragments).\n",
    "- Lack of grounding (no live access to reality unless tools/retrieval are added).\n",
    "- Pressure from prompts (‚Äúbe decisive / concise‚Äù) discourages hedging.\n",
    "- Long context drift‚Äîearlier small inaccuracies compound.\n",
    "\n",
    "Better framing:\n",
    "- Human: ‚ÄúI believed X; new evidence shows I was wrong.‚Äù (belief revision)\n",
    "- LLM: ‚ÄúGenerated sequence Y that was locally probable but factually incorrect.‚Äù (prediction error)\n",
    "\n",
    "How to reduce impact:\n",
    "- Ask for sources or evidence; require it to cite or retrieve, not just assert.\n",
    "- Use retrieval augmentation (attach a trusted document set).\n",
    "- Enable tool use (calculators, code execution) for verifiable steps.\n",
    "- Prompt for reasoning then answer (chain-of-thought or self-check) to expose inconsistencies.\n",
    "- Add a second pass: ‚ÄúVerify the previous answer; list any parts that may be wrong.‚Äù\n",
    "- Lower temperature for factual Q&A; add explicit instruction: ‚ÄúIf unsure, say you‚Äôre unsure.‚Äù\n",
    "\n",
    "Rule of thumb: Don‚Äôt treat confident style as truth. Treat every unsupported claim as a draft that needs verification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f4bae",
   "metadata": {},
   "source": [
    "3. Do you think LLMs will replace programmers, or just change the way programming is done? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac7bb2",
   "metadata": {},
   "source": [
    "LLMs won‚Äôt fully replace programmers soon; they will reshape the workflow and raise the baseline of what one person can build.\n",
    "\n",
    "What gets automated first:\n",
    "- Boilerplate: CRUD endpoints, config files, tests scaffolds, data class definitions.\n",
    "- Translation: Converting specs ‚Üí code skeletons; one language ‚Üí another.\n",
    "- Repetitive refactors: Renaming, extracting functions, lint fixes.\n",
    "- Exploratory drafting: ‚ÄúShow me 3 approaches‚Äù for an algorithm or API usage.\n",
    "\n",
    "What remains deeply human (for now):\n",
    "- Problem framing: Deciding *what* to build, trade‚Äëoffs, user value, constraints.\n",
    "- Systems & architecture: Partitioning, scalability, latency, failure domains, observability.\n",
    "- Cross-cutting concerns: Security models, privacy guarantees, regulatory compliance.\n",
    "- Debugging novel failures: Multi-service race conditions, subtle memory/perf leaks.\n",
    "- Maintaining conceptual integrity: Keeping codebase coherent as features accumulate.\n",
    "- Ethical & risk judgment: Data handling, model misuse, safety boundaries.\n",
    "\n",
    "Why full replacement is hard:\n",
    "- Ambiguity: Specs are rarely complete; humans resolve fuzzy requirements.\n",
    "- Hidden context: Legacy decisions, organizational constraints, unwritten norms.\n",
    "- Non-code work: Meetings, persuasion, design reviews, mentoring, prioritization.\n",
    "- Multi-modal reasoning: Code + infra + budget + timeline + stakeholder politics.\n",
    "\n",
    "How roles shift:\n",
    "- Junior dev tasks compress; learning shifts from ‚Äútype syntax‚Äù to ‚Äúevaluate and adapt AI output.‚Äù\n",
    "- Mid-level devs amplify output: more feature spikes, faster iterations.\n",
    "- Senior/lead engineers spend more time on architecture, quality gates, guardrails, enabling teams with AI workflows.\n",
    "\n",
    "New critical meta-skill set:\n",
    "1. Prompting as specification (clear constraints, style, test expectations).\n",
    "2. Verification mindset (test-first, diff review, security scanning, benchmarks).\n",
    "3. Tool chaining (LLM + search + static analysis + runtime traces).\n",
    "4. Data stewardship (curating internal code/context for private model usage).\n",
    "5. Governance (preventing leakage of secrets / licenses / PII through prompts).\n",
    "\n",
    "Analogy:\n",
    "- Calculators didn‚Äôt eliminate mathematicians; they removed arithmetic drudgery.\n",
    "- LLMs are ‚Äúuniversal autocomplete + contextual tutor‚Äù reducing translation friction between intent and code.\n",
    "\n",
    "Practical takeaway:\n",
    "- Programmers who lean into orchestration, auditing, and higher-level design become more valuable.\n",
    "- Those who only hand‚Äëtranslate plain-language instructions into obvious code risk displacement.\n",
    "\n",
    "So: Not replacement‚Äîaugmentation that shifts the center of gravity from writing lines to designing, validating, and integrating systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb03e9",
   "metadata": {},
   "source": [
    "4. Humans learn language from relatively little data compared to LLMs, which need billions of words. Why do you think humans are so much more data-efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9072a14",
   "metadata": {},
   "source": [
    "Humans are vastly more data‚Äëefficient than LLMs because we learn with rich signals, built‚Äëin priors, and active interaction‚Äînot just passive text prediction.\n",
    "\n",
    "Key reasons:\n",
    "- Multi-modal grounding: Babies map words to sights, sounds, touch, actions, emotions all at once. One experience = many linked signals (dense supervision). LLMs see only symbol sequences.\n",
    "- Innate priors / architecture: Evolution pre-loads biases for faces, agency detection, object permanence, causal reasoning, social attention‚Äîshrinking the hypothesis space.\n",
    "- Active learning / curiosity: Humans probe the environment, ask ‚Äúwhy?‚Äù, repeat, vary context‚Äîchoosing the next data point to maximize learning. LLMs passively absorb a fixed corpus.\n",
    "- Embodied feedback loops: Consequences (success, pain, surprise) shape memory salience. Text corpora treat all tokens equally unless manually weighted.\n",
    "- Compositional & causal modeling: Humans build structured mental models (objects, intentions, physics). This lets us generalize from a few examples. LLMs rely on statistical surface patterns without explicit world models.\n",
    "- Social scaffolding: Caregivers simplify, correct, expand (‚Äúchild-directed speech‚Äù), providing tailored gradient signals. LLM training is mostly undifferentiated bulk text.\n",
    "- Memory mechanisms: Humans consolidate, prune, and abstract; one example can reorganize existing concepts. LLMs largely memorize distributional correlations until scale smooths patterns.\n",
    "- Goal-directed context: We attach meaning and motivation (needs, rewards, plans) to language; relevance filtering accelerates retention.\n",
    "\n",
    "Analogy:\n",
    "- Human learning = high-bandwidth, adaptive tutoring with built-in theory hints.\n",
    "- LLM training = reading a shuffled encyclopedia with no questions asked and no real-time experimentation.\n",
    "\n",
    "Implication:\n",
    "- To narrow the gap, we add tools: multimodal inputs, grounded simulation, reinforcement, retrieval, memory modules, and active querying. Each pushes models a bit closer to how humans compress experience into understanding.\n",
    "\n",
    "Summary: Humans extract structured, causal, and socially guided representations from a few rich interactions; LLMs need billions of examples because they learn broad statistical form without embodied, goal-driven grounding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c628d5",
   "metadata": {},
   "source": [
    "5. When an LLM generates a poem or a story, would you call it ‚Äòcreative‚Äô? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c34477",
   "metadata": {},
   "source": [
    "I‚Äôd call it derivative or combinational creativity‚Äînot original creativity in the human sense.\n",
    "\n",
    "Why it can feel creative:\n",
    "- Novel combinations: It blends patterns, styles, metaphors from vast training data in ways a single human might not recall.\n",
    "- Style transfer: Can mimic Shakespeare + sci‚Äëfi + haiku constraints convincingly.\n",
    "- Constraint satisfaction: Adapts meter, rhyme, alliteration, tone quickly.\n",
    "\n",
    "What it lacks (so it‚Äôs not human-level creativity):\n",
    "- Intentionality: No inner goal like ‚Äúchallenge a genre‚Äù or ‚Äúexpress grief.‚Äù It responds to prompt constraints only.\n",
    "- Lived experience: No emotional substrate‚Äîdescriptions of loss are pattern echoes, not felt states.\n",
    "- Autonomy & iteration purpose: Doesn‚Äôt self-initiate revision to align with a vision; it only regenerates on request.\n",
    "- Value judgment: Cannot internally evaluate cultural impact, originality risk, or ethics of its output.\n",
    "- Long-horizon narrative stewardship: Weak at maintaining deep thematic arcs without external guidance.\n",
    "\n",
    "Types of creativity (simplified):\n",
    "- Exploratory: Search within a known style space ‚Üí LLMs are good here.\n",
    "- Combinational: Merge existing concepts ‚Üí also strong.\n",
    "- Transformational (redefining the rules) ‚Üí currently poor (needs meta-level critique + cultural context + intent).\n",
    "\n",
    "Human vs LLM process:\n",
    "- Human: Motivation ‚Üí concept formation ‚Üí drafting ‚Üí reflective editing ‚Üí purpose-driven refinement.\n",
    "- LLM: Prompt ‚Üí probabilistic token sampling shaped by learned distribution ‚Üí optional regeneration.\n",
    "\n",
    "Edge cases where it surprises us:\n",
    "- High-temperature sampling can stumble into unusual imagery; we misinterpret randomness as deep originality.\n",
    "\n",
    "How to harness it productively:\n",
    "- Use it for brainstorming variations and overcoming blank-page paralysis.\n",
    "- Then apply human filtering for voice, authenticity, coherence, ethical framing.\n",
    "- Provide explicit constraints (‚Äútheme of impermanence,‚Äù ‚Äúinvert clich√©,‚Äù ‚Äúavoid common rhyme pairs‚Äù) to steer beyond bland averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. If you ask the same LLM the same question multiple times, you might get different answers. What does this tell us about how LLMs generate text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede71743",
   "metadata": {},
   "source": [
    "Different answers show that generation is probabilistic sampling over many plausible continuations‚Äînot retrieval of a single stored fact.\n",
    "\n",
    "What‚Äôs happening under the hood:\n",
    "- Token-by-token sampling: At each step the model produces a probability distribution; we pick one token (often with temperature/top-k/top-p filtering). Small early differences cascade into divergent full answers.\n",
    "- Multiple valid paraphrases: Many phrasings encode the ‚Äúsame‚Äù idea; the model doesn‚Äôt care which as long as it fits statistical patterns.\n",
    "- Underspecified prompt: If your question leaves room (no style, length, perspective), stochastic variation fills the gap.\n",
    "- No deterministic internal truth table: For fuzzy or long-tail topics, the model‚Äôs learned distribution may have several moderately likely paths, none guaranteed correct.\n",
    "\n",
    "Why variation matters:\n",
    "- Strength: Diversity can surface alternative angles or details you didn‚Äôt think to ask for.\n",
    "- Weakness: Inconsistency can hide hallucinations‚Äîconfidence tone doesn‚Äôt signal reliability.\n",
    "\n",
    "How to reduce variance when you want consistency:\n",
    "- Set temperature = 0 (or very low) to pick the highest-probability token each time.\n",
    "- Add explicit constraints: ‚ÄúAnswer in 3 bullet points focusing on X.‚Äù\n",
    "- Provide grounding context (docs, retrieved passages) to narrow the distribution.\n",
    "- Ask for structured schema (JSON fields) to limit stylistic drift.\n",
    "\n",
    "How to harness variance when you want breadth:\n",
    "- Use higher temperature (0.7‚Äì1.0) and sample multiple candidates; then rank or ensemble them.\n",
    "- Prompt for ‚ÄúGive 3 distinct approaches‚Äù to encourage separation in one pass.\n",
    "- Aggregate consensus: Compare answers; keep facts appearing in the majority.\n",
    "\n",
    "Rule of thumb:\n",
    "- Deterministic needs (calculations, API generation) ‚Üí low temperature + structure.\n",
    "- Ideation, brainstorming, stylistic exploration ‚Üí higher temperature + multiple samples.\n",
    "\n",
    "Takeaway: Variation isn‚Äôt randomness for its own sake‚Äîit‚Äôs the model expressing the probabilistic nature of language patterns learned from diverse data. Control or leverage it depending on your goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6eb41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùó **Question 3**: What are APIs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2479b1",
   "metadata": {},
   "source": [
    "üëâ API means Application Programming Interface.\n",
    "\n",
    "That sounds big, but here‚Äôs the simple meaning:\n",
    "\n",
    "An API is like a waiter in a restaurant. üçΩÔ∏è\n",
    "\n",
    "* You (the customer) tell the waiter what food you want.\n",
    "* The waiter takes your order to the kitchen (the system).\n",
    "* The kitchen makes the food.\n",
    "* The waiter brings the food back to you.\n",
    "\n",
    "üëâ In the same way:\n",
    "\n",
    "* You (the user) send a request through an API.\n",
    "* The computer/server prepares the data.\n",
    "* The API brings the data back to you.\n",
    "\n",
    "üñ•Ô∏è Example:\n",
    "\n",
    "* If you want to know today‚Äôs **weather**, you ask the **Weather API**.\n",
    "* The API goes to the weather database, finds the answer, and gives it back to you.\n",
    "* So yophone ur app shows: *‚ÄúIt‚Äôs 30¬∞C and sunny ‚òÄÔ∏è.‚Äù*\n",
    "\n",
    "\n",
    "üåà Easy way to remember:\n",
    "\n",
    "API = Middleman/helper -> that carries your request and brings back the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùó **Question 4**: What is role of APIs in Automation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284fd0c9",
   "metadata": {},
   "source": [
    "üåü Role of APIs in Automation\n",
    "\n",
    "üëâ Automation means making machines do work automatically (without humans doing every step).\n",
    "\n",
    "üëâ APIs help automation by letting different programs talk to each other and share information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05768a01",
   "metadata": {},
   "source": [
    "üñ•Ô∏è Example 1: Using Weather API\n",
    "\n",
    "* Imagine you want to see the weather every morning.\n",
    "* Without API ‚Üí You would open the website, search, and read.\n",
    "* With API ‚Üí A program can **automatically** ask the weather API and show:\n",
    "\n",
    "  *‚ÄúGood morning! Today is sunny ‚òÄÔ∏è.‚Äù*\n",
    "  \n",
    "  ‚û°Ô∏è You don‚Äôt have to do anything ‚Äî it‚Äôs automatic!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31d6be",
   "metadata": {},
   "source": [
    "üß† Example 2: Accessing LLMs (like ChatGPT) through API\n",
    "\n",
    "* LLM (Large Language Model) is like a **big smart brain** on a server.\n",
    "* But that brain doesn‚Äôt live on your computer ‚Äî it‚Äôs far away (in the cloud).\n",
    "* **API** is the bridge üåâ that lets your app talk to that brain.\n",
    "\n",
    "üëâ Steps:\n",
    "\n",
    "1. You type a question in your app ‚Üí ‚ÄúTell me a story about a dragon.‚Äù üêâ\n",
    "2. Your app sends this request through the **API** to the LLM.\n",
    "3. The LLM thinks and writes a story.\n",
    "4. The **API brings back** the story to your app.\n",
    "\n",
    "‚û°Ô∏è This way, you can use the smart LLM **remotely** (from anywhere) without installing it on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ade04",
   "metadata": {},
   "source": [
    "üåà Easy way to remember\n",
    "\n",
    "**API is like a magic pipe** üîå:\n",
    "\n",
    "* One side: You put in your request.\n",
    "* Other side: You get back the answer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27abf4f",
   "metadata": {},
   "source": [
    "### üí° **Quick Questions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If an AI model is huge and cannot run on your laptop, how can an API make it usable for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f6131",
   "metadata": {},
   "source": [
    "An API lets you use the remote model like a service instead of running it locally.\n",
    "\n",
    "How it helps:\n",
    "- Heavy compute stays on provider GPUs (you just send text + get results).\n",
    "- You only download small request/response JSON, not 10s‚Äì100s of GB of weights.\n",
    "- Provider handles scaling, batching, optimization, security patches.\n",
    "- You pay per call (or quota) instead of buying expensive hardware.\n",
    "\n",
    "What you do:\n",
    "1. Get an API key (auth).\n",
    "2. Send an HTTPS request (prompt + parameters) to the provider endpoint.\n",
    "3. Receive structured output (text, embeddings, etc.).\n",
    "4. Integrate it into your app (UI, automation, analysis).\n",
    "\n",
    "Analogy:\n",
    "- Like streaming a movie vs. storing the whole video library at home.\n",
    "\n",
    "So: The API is the bridge that turns a huge remote model into a lightweight function call in your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76728b8c",
   "metadata": {},
   "source": [
    "2. Why do most AI companies (like OpenAI, Hugging Face, Google) provide APIs instead of letting people directly download their models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12246325",
   "metadata": {},
   "source": [
    "- Protect intellectual property & competitive edge:\n",
    "  - Training costs (tens of millions of dollars) + proprietary data/process tricks ‚Üí weights are valuable assets.\n",
    "  - API keeps the *capability* available without handing over the raw artifact that can be cloned or fine‚Äëtuned away from their ecosystem.\n",
    "\n",
    "- Safety, abuse monitoring, and policy enforcement:\n",
    "  - Centralized serving lets providers apply content filters, rate limits, provenance tagging, model usage auditing.\n",
    "  - If misuse patterns emerge (spam, disinformation, prohibited content), they can intervene globally (tweak safety layers, block keys) instantly.\n",
    "\n",
    "- Fast iteration & silent upgrades:\n",
    "  - Models get patched (quality, safety, efficiency) without users re‚Äëdownloading 100+ GB each time.\n",
    "  - Bug fixes + alignment improvements roll out seamlessly ‚Üí lower maintenance burden for developers.\n",
    "\n",
    "- Operational efficiency & performance tuning:\n",
    "  - Providers can run quantized / sharded / specialized kernels (FlashAttention, custom inference chips) behind the scenes.\n",
    "  - Dynamic batching & caching reduce latency and cost; hard to replicate locally.\n",
    "\n",
    "- Usage‚Äëbased monetization & cost recovery:\n",
    "  - API metering (per token / per request) aligns revenue with consumption.\n",
    "  - Direct downloads would enable unlimited local inference after a one‚Äëtime leak.\n",
    "\n",
    "- Security & compliance:\n",
    "  - Central control helps enforce regional data handling, logging, and audit requirements (e.g., GDPR, SOC 2, HIPAA variants where applicable).\n",
    "  - Easier to offer enterprise features: encryption in transit, audit trails, abuse detection.\n",
    "\n",
    "- Prevent downstream uncontrolled fine‚Äëtuning:\n",
    "  - Open weights can be modified to remove safeguards; API access preserves guardrails.\n",
    "\n",
    "- Telemetry for continual improvement:\n",
    "  - Aggregated (often anonymized) usage stats guide optimization, detect failure modes, prioritize new features.\n",
    "\n",
    "- Hardware abstraction for users:\n",
    "  - Many developers lack GPUs or the ops skill to host large models (memory fragmentation, scaling, auto‚Äëhealing). API = simple HTTPS call.\n",
    "\n",
    "- Licensing & data provenance constraints:\n",
    "  - Some training data licenses or safety commitments disallow redistributing weights; serving via API can stay within negotiated terms.\n",
    "\n",
    "When downloads do happen:\n",
    "- Smaller / older / research or open governance models (e.g., Llama variants, Mistral, Stable Diffusion) to foster ecosystem innovation, reproducibility, academic study.\n",
    "\n",
    "Trade‚Äëoff for developers:\n",
    "- Pros: Zero setup, always updated, scalable, safer by default.\n",
    "- Cons: Ongoing cost, dependency on provider uptime/policies, limited transparency, potential data sensitivity concerns (mitigated via enterprise agreements or on‚Äëprem variants).\n",
    "\n",
    "Rule of thumb:\n",
    "- Frontier, fast‚Äëchanging, high‚Äërisk models ‚Üí API first.\n",
    "- Commodity / commoditizing models ‚Üí more likely to see open weights.\n",
    "\n",
    "So: APIs balance access + control‚Äîletting people build quickly while providers safeguard quality, safety, and business viability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ac81d",
   "metadata": {},
   "source": [
    "3. Why do you think API rate limits exist? How does it affect AI usage?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42dcb5",
   "metadata": {},
   "source": [
    "API rate limits exist to protect the system and create fair, sustainable access.\n",
    "\n",
    "Main reasons:\n",
    "- Prevent overload: Sudden spikes (or accidental infinite loops) could overwhelm servers; limits keep latency stable for everyone.\n",
    "- Fair sharing: A few heavy users shouldn‚Äôt starve everyone else‚Äôs requests.\n",
    "- Cost control: Each call burns compute (GPU time, energy). Capping throughput keeps provider costs predictable.\n",
    "- Abuse & security mitigation: Throttles bots scraping, brute‚Äëforce attacks, spam generation, or mass content farming.\n",
    "- Quality of service (QoS): Smoothing traffic prevents cascade failures and keeps response times consistent.\n",
    "- Capacity planning signal: Patterns in rate limit hits show where to scale infra or optimize models.\n",
    "- Encourages efficient design: Forces developers to cache, batch, and debounce instead of spamming requests.\n",
    "\n",
    "How it affects AI usage:\n",
    "- Forces batching & streaming: Instead of 50 tiny calls, you combine them or request streamed output.\n",
    "- Encourages prompt optimization: Craft one well-structured request rather than many iterative micro prompts.\n",
    "- Introduces backoff logic: Clients must handle 429 errors gracefully (retry later, exponential backoff). \n",
    "- Shapes product features: Teams may pre-compute embeddings or summaries offline to stay under limits.\n",
    "- Tiers & pricing: Higher paid tiers raise limits; architecture must anticipate multiple tiers.\n",
    "\n",
    "Typical signals you see:\n",
    "- 429 Too Many Requests response.\n",
    "- Headers like: X-RateLimit-Limit, X-RateLimit-Remaining, Retry-After.\n",
    "\n",
    "Good practices to stay within limits:\n",
    "1. Cache deterministic results (same prompt/context ‚Üí reuse answer temporarily).\n",
    "2. Batch multiple embeddings or classification items into one call where API allows.\n",
    "3. Debounce user keystrokes (wait for pause before sending a request in a live UI).\n",
    "4. Use streaming for long answers instead of polling multiple times.\n",
    "5. Respect Retry-After; implement exponential backoff + random jitter.\n",
    "\n",
    "Rule of thumb:\n",
    "- Design assuming limits will occasionally be hit ‚Üí treat 429 as routine control flow, not an exceptional crash.\n",
    "\n",
    "Rate limits are not arbitrary barriers‚Äîthey‚Äôre guardrails that keep the platform reliable, fair, and economically viable while nudging developers toward efficient patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ad685",
   "metadata": {},
   "source": [
    "4. How would you handle an API error like 429 Too Many Requests in your code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6024c",
   "metadata": {},
   "source": [
    "Handling a 429 Too Many Requests error = pause + retry smartly instead of crashing or spamming.\n",
    "\n",
    "Core idea:\n",
    "- 429 means: ‚ÄúYou hit the allowed request rate. Slow down and try again later.‚Äù\n",
    "\n",
    "Key steps in code:\n",
    "1. Detect it: Check response.status_code == 429.\n",
    "2. Respect Retry-After: If the response headers contain Retry-After (seconds or HTTP date), sleep that long.\n",
    "3. If no header: Use exponential backoff (e.g., 1s, 2s, 4s, 8s) with a max cap plus random jitter (to avoid thundering herd).\n",
    "4. Limit retries: Stop after N attempts and surface a clean error to caller/log.\n",
    "5. Log context: Endpoint, prompt size, user action‚Äîhelps you tune usage patterns.\n",
    "6. Reduce future pressure: Batch, cache, debounce user input, or upgrade plan.\n",
    "\n",
    "Python sketch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7f242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, requests\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "BASE_DELAY = 1.0  # seconds\n",
    "\n",
    "def call_api_with_backoff(url, payload, headers):\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        resp = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json()\n",
    "\n",
    "        if resp.status_code == 429:\n",
    "            retry_after = resp.headers.get(\"Retry-After\")\n",
    "            if retry_after:\n",
    "                try:\n",
    "                    delay = float(retry_after)\n",
    "                except ValueError:\n",
    "                    delay = BASE_DELAY  # fallback if header is a date\n",
    "            else:\n",
    "                # exponential backoff with jitter\n",
    "                delay = min(BASE_DELAY * (2 ** (attempt - 1)), 30)  # cap at 30s\n",
    "                delay += random.uniform(0, 0.25 * delay)\n",
    "            print(f\"Rate limited. Sleeping {delay:.2f}s (attempt {attempt})...\")\n",
    "            time.sleep(delay)\n",
    "            continue\n",
    "\n",
    "        # Transient server errors (optional handling)\n",
    "        if resp.status_code in (500, 502, 503, 504) and attempt < MAX_RETRIES:\n",
    "            delay = min(BASE_DELAY * (2 ** (attempt - 1)), 20)\n",
    "            time.sleep(delay)\n",
    "            continue\n",
    "\n",
    "        # Non-retryable error\n",
    "        raise RuntimeError(f\"API error {resp.status_code}: {resp.text}\")\n",
    "\n",
    "    raise RuntimeError(\"Exceeded retry attempts due to repeated 429 responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10407888",
   "metadata": {},
   "source": [
    "Best practices:\n",
    "- Centralize this logic (don‚Äôt duplicate in every call site).\n",
    "- Monitor how often you hit 429; if frequent, optimize or raise your quota.\n",
    "- Combine small tasks (batch embeddings) to reduce call count.\n",
    "- Cache identical deterministic results for a short TTL.\n",
    "- Add circuit breaker: If many consecutive 429s, pause broader traffic.\n",
    "\n",
    "User experience angle:\n",
    "- For interactive apps, show a gentle message: ‚ÄúCooling off for a moment‚Äîretrying‚Ä¶‚Äù\n",
    "- Avoid freezing UI with no feedback.\n",
    "\n",
    "Rule of thumb:\n",
    "- Treat 429 as normal flow control, not an exception to ignore; patience + backoff keeps you reliable and courteous to shared infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019ae9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü How it all connects ü§ù\n",
    "\n",
    "* **AI** = The smart brain.\n",
    "* **LLM** = A type of AI that talks like humans.\n",
    "* **API** = The messenger that connects us to AI/LLMs.\n",
    "* **Automation** = The reason we use APIs ‚Äî to make work happen without us doing it again and again.\n",
    "\n",
    "‚ú® So, whenever you hear:\n",
    "\n",
    "* **AI** ‚Üí Smart brain for computers.\n",
    "* **LLM** ‚Üí AI that talks like humans.\n",
    "* **API** ‚Üí Messenger/waiter/helper.\n",
    "* **Automation** ‚Üí Work done automatically.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2846e7",
   "metadata": {},
   "source": [
    "## üêç Python for API Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f1820a",
   "metadata": {},
   "source": [
    "üåü **1. Using requests to call an API**\n",
    "\n",
    "Python has a library called requests that helps us talk to APIs (send requests and get answers).\n",
    "\n",
    "üëâ Example:\n",
    "Let‚Äôs get some data from a public API (e.g., a joke API ü§≠)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbebb5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\":\"general\",\"setup\":\"Have you heard the rumor going around about butter?\",\"punchline\":\"Never mind, I shouldn't spread it.\",\"id\":111}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Send a GET request to the API\n",
    "response = requests.get(\"https://official-joke-api.appspot.com/random_joke\")\n",
    "\n",
    "# Print raw response (text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb289c",
   "metadata": {},
   "source": [
    "‚û°Ô∏è This sends a message to the API, and the API replies with data (usually in JSON format)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28b497",
   "metadata": {},
   "source": [
    "üåü **2. What is JSON?**\n",
    "\n",
    "üëâ JSON = JavaScript Object Notation.\n",
    "\n",
    "It‚Äôs a way to store and share data.\n",
    "\n",
    "Looks like a dictionary in Python ‚Üí with keys and values.\n",
    "\n",
    "üí° Example JSON from joke API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b792eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 123,\n",
       " 'type': 'general',\n",
       " 'setup': 'Why did the computer go to the doctor?',\n",
       " 'punchline': 'Because it caught a virus!'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"id\": 123,\n",
    "  \"type\": \"general\",\n",
    "  \"setup\": \"Why did the computer go to the doctor?\",\n",
    "  \"punchline\": \"Because it caught a virus!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e3cd1",
   "metadata": {},
   "source": [
    "üåü **3. Handling JSON in Python**\n",
    "\n",
    "We use .json() method to turn the API response into a Python dictionary.\n",
    "\n",
    "üëâ Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7028a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup: Did you hear about the guy who invented Lifesavers?\n",
      "Punchline: They say he made a mint.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Call the API\n",
    "response = requests.get(\"https://official-joke-api.appspot.com/random_joke\")\n",
    "\n",
    "# Convert to Python dictionary\n",
    "data = response.json()\n",
    "\n",
    "# Access parts of the JSON\n",
    "print(\"Setup:\", data[\"setup\"])\n",
    "print(\"Punchline:\", data[\"punchline\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f71a2",
   "metadata": {},
   "source": [
    "‚û°Ô∏è The API replies back showing it got your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4b744",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Activities: APIs + Python + LLMs**\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Activity 1: Set up Python Environment**\n",
    "\n",
    "Students can choose **Google Colab** (easiest) or **Local Setup**.\n",
    "\n",
    "**Option A: Google Colab (recommended üéâ)**\n",
    "\n",
    "1. Go to [Google Colab](https://colab.research.google.com/).\n",
    "2. Click **New Notebook**.\n",
    "3. You‚Äôre ready to write Python in the cloud ‚úÖ.\n",
    "\n",
    "**Option B: Local Setup (for advanced students)**\n",
    "\n",
    "1. Install **Anaconda** ‚Üí it comes with **Jupyter Notebook**.\n",
    "2. Open **Jupyter Notebook** and start coding.\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Activity 2: Sign Up for Groq API**\n",
    "\n",
    "1. Go to [console.groq.com](https://console.groq.com).\n",
    "2. Make a **free account**.\n",
    "3. Find your **API Key** (a secret password for using Groq).\n",
    "\n",
    "   * Copy it ‚Üí we‚Äôll use it in Python.\n",
    "   * üîë Keep it safe! Never share it publicly.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0f161",
   "metadata": {},
   "source": [
    "# üìò Groq account, Notebook and GitHub üóÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a92ee",
   "metadata": {},
   "source": [
    "### Part 1: Google Colab (Easiest Path)\n",
    "\n",
    "1. Go to [Google Colab](https://colab.research.google.com/).\n",
    "2. Click **New Notebook** ‚Üí a fresh page opens.\n",
    "3. In each cell, type Python code ‚Üí press **Shift + Enter** to run.\n",
    "4. Install libraries inside Colab with `!pip install <package>`\n",
    "\n",
    "üí° Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eca7d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: groq in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.31.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from groq) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from groq) (2.11.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\junai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\junai\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7df5b",
   "metadata": {},
   "source": [
    "### Part 2: Local Setup with Jupyter Notebook\n",
    "\n",
    "**üîπ Step 1: Install Anaconda**\n",
    "\n",
    "* Download [Anaconda](https://www.anaconda.com/products/distribution).\n",
    "* Install it (comes with Python + Jupyter).\n",
    "\n",
    "**üîπ Step 2: Open Jupyter Notebook**\n",
    "\n",
    "* Open **Anaconda Navigator**.\n",
    "* Click **Launch Jupyter Notebook**.\n",
    "* A browser window will open at: `http://localhost:8888/tree`.\n",
    "\n",
    "**üîπ Step 3: Create Notebook**\n",
    "\n",
    "* Click **New ‚Üí Python 3 Notebook**.\n",
    "* Now write Python code in cells and press **Shift + Enter** to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c167eb",
   "metadata": {},
   "source": [
    "### Part 3: GitHub + Notebooks\n",
    "\n",
    "**üîπ Step 1: Install Git (only once)**\n",
    "\n",
    "* [Download Git](https://git-scm.com/downloads).\n",
    "\n",
    "**üîπ Step 2: Setup GitHub Account**\n",
    "\n",
    "* Sign up at [github.com](https://github.com).\n",
    "* Create a new repository (e.g., `AI-Projects`).\n",
    "\n",
    "**üîπ Step 3: Connect Local Notebook to GitHub**\n",
    "\n",
    "In terminal (or Anaconda Prompt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to your project folder\n",
    "cd my_notebooks\n",
    "\n",
    "# Initialize Git\n",
    "git init\n",
    "\n",
    "# Add remote repository (replace with your repo link)\n",
    "git remote add origin https://github.com/username/AI-Projects.git\n",
    "\n",
    "# Add your files\n",
    "git add .\n",
    "\n",
    "# Commit changes\n",
    "git commit -m \"First notebook\"\n",
    "\n",
    "# Push to GitHub\n",
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d82863",
   "metadata": {},
   "source": [
    "\n",
    "### Upload via Website (simpler option)\n",
    "\n",
    "* Go to your repo on GitHub ‚Üí **Upload Files** ‚Üí drag & drop your `.ipynb`.\n",
    "\n",
    "### Using Colab with GitHub (Direct)\n",
    "\n",
    "Colab can **open & save notebooks directly to GitHub**:\n",
    "\n",
    "1. In Colab ‚Üí **File ‚Üí Save a copy in GitHub**.\n",
    "2. Choose your repo & folder.\n",
    "3. Done ‚úÖ\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd431536",
   "metadata": {},
   "source": [
    "# **üåüResourcesüåü**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6fce7",
   "metadata": {},
   "source": [
    "### üîπ 1. Groq API Documentation\n",
    "\n",
    "* üåç [Groq API Docs](https://console.groq.com/docs)\n",
    "* What you‚Äôll learn:\n",
    "  * How to create an API key\n",
    "  * Available models (like **Llama-3**)\n",
    "  * Code examples in Python\n",
    "  * Advanced usage (streaming, parameters, etc.)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### üîπ 2. Python `requests` Library\n",
    "\n",
    "* üåç [Requests Documentation](https://requests.readthedocs.io/en/latest/)\n",
    "* What you‚Äôll learn:\n",
    "  * How to make GET and POST requests\n",
    "  * How to handle JSON responses\n",
    "  * How to send headers & parameters\n",
    "  * Real-world API examples\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### üîπ 3. JSON Basics (extra help)\n",
    "\n",
    "* üåç [W3Schools JSON Tutorial](https://www.w3schools.com/js/js_json_intro.asp)\n",
    "* Simple, beginner-friendly intro to JSON (how data is structured, keys/values, etc.).\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## üîπ 4. GitHub Guides (for sharing notebooks)\n",
    "\n",
    "* üåç [GitHub Docs: Hello World](https://docs.github.com/en/get-started/quickstart/hello-world)\n",
    "* üåç [Uploading files to GitHub](https://docs.github.com/en/repositories/working-with-files/managing-files/adding-a-file-to-a-repository)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
